<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="utf-8" />
<title>Michael (Mikhail) V. Yudelson's Academic Page</title>
<link rel="stylesheet" href="css/reset.css" />
<link rel="stylesheet" href="css/text.css" />
<link rel="stylesheet" href="css/960.css" />
<link rel="stylesheet" href="css/base.css" />
<link rel="stylesheet" href="css/special.css" />
<link rel="shortcut icon" type="image/png" href="/favicon.png"/>
<link rel="shortcut icon" type="image/png" href="http://yudelson.info/favicon.png"/>
</head>
<body>
<div class="container_12">
	<!-- Large header -->
	<div class="grid_12 title_tab_large grey" id="menu_top">
	  <p>Michael (Mikhail) V. Yudelson's Academic Page</p>
	</div>
	<div class="clear"></div>
	<!-- menu bar-->
	<div class="grid_8" id="menu_middle">
	  <p class="top_tab">
			<span><a href="index.html"><img src="img/home_256_black.png" style="vertical-align:text-top;" height="20" width="20"/></a></span>&nbsp;&nbsp;<span><a href="vitae.html">Vitae</a></span>&nbsp;&nbsp;<span><a href="proj.html">Projects</a></span>&nbsp;&nbsp;<span><a href="pub.html">Publications</a></span>
		</p>
	</div>
	<div class="grid_4" id="menu_social">
		<a href="http://www.facebook.com/michael.yudelson" target="_blank"><img src="img/facebook_40_black.png" height="35" width="35"/></a>&nbsp;
		<a href="http://twitter.com/myudelson" target="_blank"><img src="img/twitter_40_black.png" height="35" width="35"/></a>
		<a href="http://www.linkedin.com/in/michael1yudelson" target="_blank"><img src="img/linkedin_40_black.png" height="35" width="35"/></a>
		<a href="https://github.com/myudelson" target="_blank"><img src="img/github_40_black.png" height="35" width="35"/></a>
		<a href="https://scholar.google.com/citations?user=ncrZ-ZsAAAAJ&hl=en" target="_blank"><img src="img/googlescholar_256_black.png" height="35" width="35"/></a>
		<a href="http://orcid.org/0000-0002-8280-0348" target="_blank"><img src="img/orcid_64_black.png" height="35" width="35"/></a>
		<a href="http://dblp.uni-trier.de/pers/hd/y/Yudelson:Michael.html" target="_blank"><img src="img/dblp_120_black.png" height="35" width="35"/></a>
	</div>
	<div class="clear"></div>

 	<div class="grid_12">
		<h1>hmm-scalable</h1>

		<p>This tool is developed to fit Bayesian Knowledge Training models efficiently on large datasets. It is a command line utility written in C/C++. The project started while the author was a post-doc at Human-Computer Interaction Institute, Carnegie Mellon University. These sources are published under BSD-new (3-clause BSD) license.</p>
			
		<!-- Table of contents-->
		<h2>Table of contents</h2>
		<p><a href="#bkt">Bayesian Knowledge Tracing</a><br/>
		<a href="#data">Data</a><br/>
		<a href="#train">Training BTK Models</a><br/>
		<a href="#predict">Using Models for Predictions</a><br/>
		<a href="#example">Examples</a><br/>
		<a href="#pubs">Publications Featuring <strong>hmm-scalable</strong></a><br/>
		<a href="#ref">References</a></p>

		<h2><a id="bkt"></a>Bayesian Knowledge Tracing</h2>

		<p>BKT is a user modeling approach widely use in the area of Intelligent Tutoring Systems (ITS). In ITS, it is customary to tag problems and problem steps students are solving with knowledge quanta often called skills. When a student is attempting to solve a problem or a problem step, they are said to <em>practice a skill</em>. A student model in a general sense performs deductions on whether the skill(s) is (are) mastered given a sequence of correct and incorrect attempts to solve a problems/problem steps that are tagged with it. The goal of BKT is to infer whether the student has mastered a skill or not from a pattern of successful or unsuccessful attempts to apply it. BKT uses a formalism of a Hidden Markov Model to estimate student's skill mastery. In BKT, there are two types of nodes: binary state nodes capture skill mastery (one per skill) that assume values of <em>mastered</em> or <em>not mastered</em> and binary observation nodes (one per skill) that assume values of <em>correct</em> or <em>incorrect</em>.</p>

		<p>For each skill, BKT has four parameters.</p>

		<ol>
		<li><em>p-init</em> or pLo - is a probability the skill was known a priori,</li>
		<li><em>p-learn</em> or pT - is a probability the skill will transition into "mastered"
		    state upon practice attempt,</li>
		<li><em>p-forget</em> or pF - is a probability that the skill will transition into "not mastered state" upon practice attempt. Traditionally, pForget is set to zero.</li>
		<li><em>p-slip</em> or pS - is a probability that a known skill is applied incorrectly, and</li>
		<li><em>p-guess</em> or pG - is a probability that an unknown skill will be applied correctly.</li>
		</ol>
			
		<p>Parameters of the BKT can be represented in the matrix form. Priors &mdash; <img src="http://latex.codecogs.com/gif.latex?%5CPi" alt="\pi"/> &mdash; are a vector 1*N, where N is the number of states (N=2 in standard BKT). Throughout this discussion, we will we assume that the first element of the <img src="http://latex.codecogs.com/gif.latex?%5CPi" alt="\pi"/> vector is the a priory probability of knowing the skill. <em>p-learn</em> is part of the transitions matrix A that captures probabilities of state changes from the row to the column and is N*N. No forgetting is captured by setting A[1,2] = 0 (from mastered to unmastered). <em>p-learn</em> corresponds to A[2,1] - from unmastered to mastered. <em>p-guess</em> and <em>p-slip</em> are specified in B &mdash; observation matrix N*M, where M is the number of observations (M=2 in standard BKT). The first column corresponds to a correct observation, the second &mdash; to the incorrect. For standard BKT with two observations, <em>p-guess</em> is B[2,1] &mdash; unmastered skill but a correct response, and <em>p-splip</em> is B[1,2] - mastered skill but an incorrect response.</p>

<p><img src="http://latex.codecogs.com/gif.latex?%5CPi%3D%0D%0A%5Cbegin%7Bbmatrix%7D%0D%0Ap%28L_0%29%261-p%28L_0%29%5C%5C%0D%0A%5Cend%7Bbmatrix%7D" alt="\Pi=\[p(L_0), 1-p(L_0)\]"/></p>

<p><img src="http://latex.codecogs.com/gif.latex?A%3D%0D%0A%5Cbegin%7Bbmatrix%7D%0D%0A1%260%5C%5C%0D%0Ap%28T%29%261-p%28T%29%0D%0A%5Cend%7Bbmatrix%7D" alt="A=\[1, 0; p(T), 1-p(T)\]"/></p>

<p><img src="http://latex.codecogs.com/gif.latex?B%3D%0D%0A%5Cbegin%7Bbmatrix%7D%0D%0A1-p%28S%29%26p%28S%29%5C%5C%0D%0Ap%28G%29%261-p%28G%29%0D%0A%5Cend%7Bbmatrix%7D" alt="B=\[1-p(S), p(S); p(G), 1-p(G)\]"/></p>


		<p>For more details on BKT refer to [<a href="#KT">1</a>]. [<a href="#IATPFMPASR">2</a>], among other things, discusses how a gradient-based fitting of HMM can be implemented. [<a href="#WC">3</a>, <a href="#NCGM">4</a>] cover additional topics relevant to the implementation.</p>

		<h2><a id="getcompile"></a>Getting and Compiling the Code</h2>

		<p>The public version of <strong>hmm-scalable</strong> code is available via <a href="http://www.educationaldatamining.org/" target="_blank">International Educational Data Mining Society</a>'s GitHub repository <a href="http://goo.gl/5DqpkW" target="_blank">here</a>. If you have <code>git</code> tool installed, use the following command to get the source code.</p>
		
		<code>git clone https://github.com/IEDMS/standard-bkt</code>

		<p>To compile issue <code>make all</code> command. If you are on Linux, the g++/gcc compiler and Open MP library should already be installed. In Mac OX, you would need command line tools of Xcode to be installed. g++/gcc compiler with Open MP (no longer bundled with Mac OS X by default) could be downloaded from <a href="hpc.sourceforge.net" target="_blank">here</a>. If you are on Windows, you might need to install <code>cygwin</code> and have the g++/gcc compiler available and be sure to install <code>make</code> command with <code>cygwin</code>.</p>

		<h2><a id="data"></a>Data</h2>

		<p>Input file data format is quite straightforward. Four tab-separated columns: observation, student, problem/problem step, skill(s). Observation is a 1-started integer. For the two-state BKT, we use 1 for <em>correct</em> and 2 for <em>incorrect</em>. The student is a string label, so is problem or problem step, whatever granularity you prefer. Skill is a string label as well. Multiple skill labels should be delimited by a character of your choice (do not use tab). An example of few lines of input is given below where tilde symbol <em>~</em> is used as a delimiter.</p>

<pre>
-- input file --
2   student_001 unit1-section1-problem5-step1  addition~multiplication
1   student_001 unit1-section1-problem5-step2  multiplication
1   student_001 unit1-section1-problem5-step3  addition
-- input file --
</pre>

		<p>If there is no skill label for a particular row of data use '.' (dot) symbol. In test data, the utility will use known observations for training and will produce predictions for missing observations that should have '.' (dot) instead of observation code (1, 2, or otherwise).</p>

		<p>Output files are the model and the predictions file. The model file contains general information about the data, e.g., the number of observations, the number of skills, and the number of problems/problem steps; as well as the model parameters.</p>

		<p>Prediction file consists of model predictions for each row of the input file. Depending on the option (see parameter specifications below), you can print out probability distribution for student's response (probability of correct and probability of incorrect, if the observation node is binary). The probability values are tab separated. See examples of a outputs file below.</p>

<pre>
-- model file --
SolverId	1.1
nK	1
nG	1
nS	2
nO	2
Null skill ratios	  1.0000000	  0.0000000
0	multiplication-skill
PI	0.45000000	0.55000000
A	1.00000000  0.00000000	0.40000000	0.60000000
B	0.79000000	0.21000000	0.22000000	0.78000000
-- model file --
</pre>

		<p>In the model file example above, we see a specification of the solver algorithm (to be discussed below), one skill (nK=1), one student in the data (nG=1), two hidden states and 2 observations (nS and nO respectively). Skill numbering is zero started. Skill name is "multiplication-skill". Rows prefixed with PI, A, and B correspond to priors, transition, and observation matrices written row by row. Thus, first element of PI is pLo=0.55, 3rd element of A is pT=0.4, 2nd and 3rd elements of B are pS=0.12 and pG=0.22 respectively.</p>
		<p>Prediction file, for a standard BKT model with nO=2 observations, would contain 2 tab-separated columns. The first column corresponds to the model-estimated prior probability of the student being correct for that particular observation. The second column is the complement of the probability of correct -- probability of being incorrect (see an example below).</p>

<pre>
-- prediction file --
0.73    0.27
0.88    0.12
0.94    0.06
0.99    0.01
-- prediction file --
</pre>
		<h2><a id="train"></a>Training BKT models</h2>

		<p>trainhmm utility has the following launch signature:</p>
<pre>
trainhmm [options] input_file [[model_file] prediction_file]
options:
-s : structure.solver[.solver setting], structures: 1-by skill, 2-by user;
    solvers: 1-Baum-Welch, 2-Gradient Descent, 3-Conjugate Gradient Descent;
    Conjugate Gradient Descent has 3 settings: 1-Polak-Ribiere,
    2-Fletcherâ€“Reeves, 3-Hestenes-Stiefel, 4-Dai-Yuan.
    For example '-s 1.3.1' would be by skill structure (classical) with
    Conjugate Gradient Descent and Hestenes-Stiefel formula, '-s 2.1' would be
    by student structure fit using Baum-Welch method.
-e : tolerance of termination criterion (0.01 default)
-i : maximum number of iterations (200 by default)
-q : quiet mode, without output, 0-no (default), or 1-yes
-n : number of hidden states, should be 2 or more (default 2)
-0 : initial parameters comma-separated for priors, transition, and emission
    probabilities skipping the last value from each vector (matrix row) since
    they should sum up to 1; default 0.5,1.0,0.4,0.8,0.2
-l : lower boundaries for parameters, comma-separated for priors, transition,
    and emission probabilities (without skips); default 0,0,1,0,0,0,0,0,0,0
-u : upper boundaries for params, comma-separated for priors, transition,
    and emission probabilities (without skips); default 1,1,1,0,1,1,1,0.3,0.3,1
    with slip and guess capped at 0.3.
-m : report model fitting metrics (AIC, BIC, RMSE) 0-no (default), 1-yes. To 
    specify observation for which metrics to be reported, list it after ','.
    For example '-m 0', '-m 1' (by default, observation 1 is assumed), '-m 1,2'
    (compute metrics for observation 2). Incompatible with '-v' option.
-v : cross-validation folds, stratification, and target state to validate
    against, folds input/output file, default 0 (no cross-validation),
    examples '-v 5,i,2' - 5 fold, item-stratified c.-v., predict state 2,
    '-v 10' - 10-fold subject-stratified c.-v. predict state 1 by default,
    alternatively '-v 10,g,1', '-v 5,n,2,folds.txt,o' - 5-fold unstratified
    c.-v. predicting state 2, [o]output folds to 'folds.txt', and here 
    '-v 5,n,2,folds.txt,i', folds are actually read [i]n from the file.
-p : report model predictions on the train set 0-no (default), 1-yes.
-d : delimiter for multiple skills per observation; single skill per observation
    (default), otherwise -- delimiter character, e.g. '-d ~'.
-b : treat input file as binary input file created from text file by
    inputconvert utility.
-B : block re-estimation of prior, transitions, or emissions parameters
    respectively (default is '-B 0,0,0'), to block re-estimation of transition
    probabilities specify '-B 0,1,0'.
-P : use parallel processing, default - 0 (no parallel processing), 1 - fit
    separate skills/students separately, 2 - fit separate sequences within
    skill/student separately.
-o : in addition to printing to console, print output to the file specified
    default is empty.
</pre>


		<h2><a id="predict"></a>Using models for prediction</h2>

		<p>predicthmm utility has the following launch signature:</p>

<pre>
predicthmm [options] input_file model_file [predicted_response_file]
options:
-q : quiet mode, without output, 0-no (default), or 1-yes
-m : report model fitting metrics (AIC, BIC, RMSE) 0-no (default), 1-yes. To 
     specify observation for which metrics to be reported, list it after ','.
     For example '-m 0', '-m 1' (by default, observation 1 is assumed), '-m 1,2'
     (compute metrics for observation 2). Incompatible with-v option.
-d : delimiter for multiple skills per observation; single skill per observation
     (default), otherwise -- delimiter character, e.g. '-d ~'.
-b : treat input file as binary input file  created from text file by
     inputconvert utility.
-p : report model predictions on the train set 0-no (default), 1-yes; works 
     with -v and -m parameters.
</pre>

		<h2><a id="example"></a>Examples</h2>

		<p>Small sample data file <toy_data.txt> is generated using the following BKT parameters: pLo=0.4, pT=0.35, pS=0.25, pG=0.12.</p>

		<p>To fit a BKT model of this data using an EM algorithm run the following command:</p>

		<code>sh> ./trainhmm -s 1.1 -m 1 -p 1 toy_data.txt model.txt predict.txt</code>

		<p>The model will have 90% accuracy and root mean squared error (RMSE) = 0.302691 and the recovered BKT parameters would be: pLo=0.00000000, pT=0.16676161, pS=0.00044059, pG=0.00038573. Overall loglikelihood, actually, goes up from 9.3763477 to 10.4379501 in 3 iterations.</p>

		<p>If we fit BKT model using Gradient Descent method using '-s 1.2' argument, the recovered parameters would be: pLo=0.00041944, pT=0.17478539, pS=0.07938036, 0.03804388, the accuracy would remain at 90% while RMSE = 0.299250. Loglikelihood changes from  9.3763477 to 6.4099682 after 11 iterations.</p>

		<p>To generate predictions using a previously fit model run the following command:</p>

		<code>sh> ./predicthmm -p 1 toy_data_test.txt model.txt predict.txt</code>

		<p>To give this tool a proper test you might want to try it on a KDD Cup 2010 dataset donated to the Pittsburgh Science of Learning Center by Carnegie Learning Inc. The dataset can be downloaded (after a quick registration) from <a href="http://pslcdatashop.web.cmu.edu/KDDCup/" target="_blank">here</a>. This datasets consists of training and challenge sets. For the sake of testing the tool, download the challenge  Algebra I set that has about 9 million transactions of over 3300 students. The training file should be trimmed to the tool's format. The command below unzips the training file and reformats it.</p>

		<div class="code">sh> unzip -q -c algebra_2008_2009.zip algebra_2008_2009_train.txt | awk "-F\t" 'BEGIN{OFS="\t"} {if(NR==1)next; skill=$20; gsub("~~", "~", skill); if(skill=="") skill="."; else { skill=$3"__"skill; gsub(/~/, "~"$3"__",skill);} } print 2-$14,$2,$3"__"$4,skill;}' > a89_train.txt</div>

		<p>To fit a BKT model of this dataset using gradient descent method as well as to  compute fit metrics and the prediction run the following command:</p>

		<code>sh> ./trainhmm -s 1.2 -d ~ -m 1 -p 1 a89_kts_train.txt model.txt predict.txt</code>

		<p>Depending on your hardware, the model should be fit in about 1-2 minutes.</p>


		<h2><a id="pubs"></a>Publications Featuring <strong>hmm-scalable</strong></h2>

		<p><a name="IBKTM"></a>Yudelson, M., Koedinger, K., Gordon, G. (2013) <strong>Individualized Bayesian Knowledge Tracing Models.</strong> In: Lane, H.C., and Yacef, K., Mostow, J., Pavlik, P.I. (eds.) Proceedings of 16th International Conference on Artificial Intelligence in Education (AIED 2013), Memphis, TN. LNCS vol. 7926, (pp. 171-180). Springer-Verlag Berlin Heidelberg (2013). [<a href="http://dx.doi.org/10.1007/978-3-642-39112-5_18" target="_blank">DOI</a>] [<a href="pdf/AIED2013_YudelsonKG.pdf" target="_blank">PDF</a>]</p>		<p>Yudelson, M. &amp; Ritter (2015) <strong>Small Improvement for the Model Accuracy &ndash; Big Difference for the Students.</strong> In: Proceedings of 17th International Conference on Artificial Intelligence in Education (AIED 2015), Madrid, Spain. [<a href="pdf/AIED2015_YudelsonR.pdf" target="_blank">PDF</a>]</p>
		<p>Falakmasir, M., Yudelson, M., Ritter, &amp; Koedinger, K. (2015) <strong>Spectral Bayesian Knowledge Tracing.</strong> In: O. Santos, J. Boticario, C. Romero, M. Pechenizkiy, A. Merceron, P. Mitros, J.M. Luna, C. Mihaescu, P. Moreno, A. Hershkovitz, S. Ventura, &amp; M. Desmarais (eds.) Proceedings of the 8th International Conference on Educational Data Mining (EDM 2015), Madrid, Spain, (pp.360-363). [<a href="pdf/EDM2015_FalakmasirYRK.pdf" target="_blank">PDF</a>]</p>			
		<p><a name="BDBBD"></a>Yudelson, M., Fancsali, S., Ritter, S., Berman, S., Nixon, T., &amp; Joshi, A. (2014) <strong>Better Data Beats Big Data.</strong> In: J. Stamper, Z. Pardos, M. Mavrikis, B. McLaren (eds.) Proceedings of the 7th International Conference on Educational Data Mining (EDM 2014), London, UK, (pp.204-208). [<a href="pdf/EDM2014_YudelsonFRBNJ.pdf" target="_blank">PDF</a>]</p>
		<p><a name="IBKTM"></a>Yudelson, M., Koedinger, K., Gordon, G. (2013) <strong>Individualized Bayesian Knowledge Tracing Models.</strong> In: Lane, H.C., and Yacef, K., Mostow, J., Pavlik, P.I. (eds.) Proceedings of 16th International Conference on Artificial Intelligence in Education (AIED 2013), Memphis, TN. LNCS vol. 7926, (pp. 171-180). Springer-Verlag Berlin Heidelberg (2013). [<a href="http://dx.doi.org/10.1007/978-3-642-39112-5_18" target="_blank">DOI</a>] [<a href="pdf/AIED2013_YudelsonKG.pdf" target="_blank">PDF</a>]</p>

		<h2><a id="ref"></a>References</h2>
		<ol>
			<li><a name="KT"></a>Corbett, A. T. and Anderson, J. R.: Knowledge tracing: Modeling the
		    acquisition of procedural know	ledge. User Modeling and User-Adapted
		    Interaction, 4(4), 253-278. (1995)</li>
			<li><a name="IATPFMPASR"></a>Levinson, S. E., Rabiner, L. R., and Sondhi, M. M.: An Introduction to the
		    Application of the Theory of Probabilistic Functions of a Markov Process to
		    Automatic Speech Recognition. Bell System Technical Journal, 62(4):
		    1035-1074. (1983)</li>
			<li><a name="WC"></a>Wolfe conditions. (2014, May 12). In Wikipedia, The Free Encyclopedia. Retrieved 23:07, July 15, 2015, from <a href="http://en.wikipedia.org/w/index.php?title=Wolfe_conditions&oldid=608251997" target="_blank">http://en.wikipedia.org/w/index.php?title=Wolfe_conditions&amp;oldid=608251997</a></li>
			<li><a name="NCGM"></a>Nonlinear conjugate gradient method. (2015, June 26). In Wikipedia, The Free Encyclopedia. Retrieved 23:08, July 15, 2015, from <a href="http://en.wikipedia.org/w/index.php?title=Nonlinear_conjugate_gradient_method&oldid=668720538" target="_blank">http://en.wikipedia.org/w/index.php?title=Nonlinear_conjugate_gradient_method&amp;oldid=668720538</a></li>
		</ol>
				
 	</div>


	<div class="clear"></div>
	<!-- Footer style="border-top:1px dotted"-->
	<div class="grid_10 grey">
		<p><small>Michael V. Yudelson &copy; 2016</small></p>
	</div>
	<div class="grid_2 grey">
		<p class="right"><small>Adapted from <a href="http://960.gs" target="_blank">960 grid</a></small></p>
	</div>
	<!-- end Footer -->

<!-- Google Analytics tracking -->
<script>
	(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
	(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
	m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
	})(window,document,'script','//www.google-analytics.com/analytics.js','ga');
	ga('create', 'UA-126816-12', 'auto');
	ga('send', 'pageview');
</script>

</body>
</html>